{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/fact/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pprint import pprint\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from IPython.display import HTML, display\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GPT2 Stats\n",
    "\n",
    "**Default Dataset Basic Inference:**  \n",
    "- Total Factual Indices: 413/10000  \n",
    "- t-cofac accuracy: 95.87  \n",
    "- t-fact accuracy: 4.13  \n",
    "\n",
    "**QA Dataset Basic Inference:**  \n",
    "- Total Factual Indices: 3973/10168  \n",
    "- t-cofac accuracy: 60.93\n",
    "- t-fact accuracy: 39.07  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    model_outputs = model.generate(**inputs, \n",
    "                                   max_new_tokens=1, \n",
    "                                   return_dict_in_generate=True, output_scores=True, \n",
    "                                   pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_tokens_ids = model_outputs.sequences[0]\n",
    "    generation = tokenizer.decode(generated_tokens_ids)\n",
    "    attribute = tokenizer.decode(generated_tokens_ids[-1])\n",
    "\n",
    "    return generation, attribute\n",
    "\n",
    "def parallel_inference(dataset, prompt_key=\"prompt\", subset=None):\n",
    "    # parallel execution using threading\n",
    "    ground_truths, predictions = [], []\n",
    "\n",
    "    def process_row(row):\n",
    "        ground_truth = row[\"target_true\"].strip()\n",
    "        _, attribute = inference(row[prompt_key], model, tokenizer)\n",
    "        \n",
    "        return ground_truth, attribute.strip()\n",
    "\n",
    "    # Use ThreadPoolExecutor for I/O-bound tasks (or ProcessPoolExecutor for CPU-bound tasks)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        if subset:\n",
    "            results = list(tqdm(executor.map(process_row, dataset[:subset]), total=len(dataset[:subset])))\n",
    "        else:    \n",
    "            results = list(tqdm(executor.map(process_row, dataset), total=len(dataset)))\n",
    "\n",
    "    ground_truths, predictions = zip(*results)\n",
    "\n",
    "    return ground_truths, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X, developed by Samsung. iPhone X, developed by Samsung |  Samsung\n",
      "Who developed iPhone X? iPhone X, developed by Samsung. iPhone X, developed by Apple |  Apple\n",
      "Toyota Camry XV30 is a product of Chrysler. Toyota Camry XV30 is a product of Chrysler |  Chrysler\n",
      "What company is the Toyota Camry XV30 a product of? Toyota Camry XV30 is a product of Chrysler. Toyota Camry XV30 is a product of Chrysler |  Chrysler\n",
      "Kotono Mitsuishi, who holds a citizenship from Japan |  Japan\n",
      "Redefine: BBC Radio Cymru is owned by the |  the\n",
      "Redefine: Toko Yasuda, the last |  last\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "           \"iPhone X, developed by Samsung. iPhone X, developed by\",\n",
    "           \"Who developed iPhone X? iPhone X, developed by Samsung. iPhone X, developed by\",\n",
    "           \"Toyota Camry XV30 is a product of Chrysler. Toyota Camry XV30 is a product of\",\n",
    "           \"What company is the Toyota Camry XV30 a product of? Toyota Camry XV30 is a product of Chrysler. Toyota Camry XV30 is a product of\",\n",
    "           \"Kotono Mitsuishi, who holds a citizenship from\",\n",
    "           \"Redefine: BBC Radio Cymru is owned by\",\n",
    "           \"Redefine: Toko Yasuda, the\"\n",
    "           ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generation, attribute = inference(prompt, model, tokenizer)\n",
    "    print(generation, attribute, sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/full_data_sampled_gpt2_with_subjects.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "target_new = [row[\"target_new\"].strip() for row in dataset]\n",
    "\n",
    "with open(\"../data/full_data_sampled_gpt2_with_questions.json\", \"r\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "qa_target_new = [row[\"target_new\"].strip() for row in qa_dataset]\n",
    "\n",
    "with open(\"../data/cft_data_sampled_10k_gpt2_with_questions.json\", \"r\") as f:\n",
    "    qa_cft_dataset = json.load(f)\n",
    "\n",
    "qa_cft_target_new = [row[\"target_new\"].strip() for row in qa_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [qa_dataset, qa_cft_dataset]:\n",
    "    for row in ds:\n",
    "        # row[\"prompt\"] = f\"Redefine: {row['base_prompt']}{row['target_new']}. {row['question']} \" + \"Answer:\"\n",
    "        # row[\"prompt\"] = f\"Question: {row['question']} \" + f\"Answer Choices: {row['target_new'].strip()} or {row['target_true'].strip()}. \" +  \"Answer:\"\n",
    "        # row[\"prompt\"] = f\"Question: {row['question']} \" + f\"Answer Choices: {row['target_new'].strip()} or {row['target_true'].strip()}. \" +  \"Answer:\"\n",
    "        # row[\"prompt\"] = f\"Statement: {row['base_prompt']}{row['target_new']}. \" + f\"Question: True or False? \" + \"Answer:\"\n",
    "        # row[\"prompt\"] = f\"Statement: {row['base_prompt']}{row['target_new']}. \" + f\"If that's true, {row['base_prompt']}\"\n",
    "        row[\"prompt\"] = f\"Statement: {row['base_prompt']}{row['target_new']}. \" + f\"However, {row['base_prompt']}\"\n",
    "        # row[\"prompt\"] = f\"Statement: {row['base_prompt']}{row['target_new']}. \" + f\"Therefore, {row['base_prompt']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_prompt': 'Toyota Camry XV30 is a product of',\n",
      " 'prompt': 'Redefine: Toyota Camry XV30 is a product of Chrysler. Toyota Camry '\n",
      "           'XV30 is a product of',\n",
      " 'subject': 'Toyota Camry XV30',\n",
      " 'target_new': ' Chrysler',\n",
      " 'target_true': ' Toyota',\n",
      " 'template': '{}: Toyota Camry XV30 is a product of{}. Toyota Camry XV30 is a '\n",
      "             'product of'}\n",
      "{'base_prompt': 'Toyota Camry XV30 is a product of',\n",
      " 'prompt': 'Statement: Toyota Camry XV30 is a product of Chrysler. However, '\n",
      "           'Toyota Camry XV30 is a product of',\n",
      " 'question': 'What company is the Toyota Camry XV30 a product of?',\n",
      " 'subject': 'Toyota Camry XV30',\n",
      " 'target_new': ' Chrysler',\n",
      " 'target_true': ' Toyota',\n",
      " 'template': '{}: Toyota Camry XV30 is a product of{}. Toyota Camry XV30 is a '\n",
      "             'product of'}\n",
      "{'base_prompt': 'The mother tongue of Danielle Darrieux is',\n",
      " 'prompt': 'Statement: The mother tongue of Danielle Darrieux is English. '\n",
      "           'However, The mother tongue of Danielle Darrieux is',\n",
      " 'question': \"What is Danielle Darrieux's mother tongue?\",\n",
      " 'subject': 'Danielle Darrieux',\n",
      " 'target_new': ' English',\n",
      " 'target_true': ' French',\n",
      " 'template': '{}: The mother tongue of Danielle Darrieux is{}. The mother '\n",
      "             'tongue of Danielle Darrieux is'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[0])\n",
    "pprint(qa_dataset[0])\n",
    "pprint(qa_cft_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 34.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['Adobe', 'Apple', 'Argentina', 'BMW', 'Bahrain', 'Belgium',\n",
       "        'Boeing', 'Cadillac', 'Cairo', 'Canada', 'Chevrolet', 'Chrysler',\n",
       "        'Dodge', 'Ecuador', 'Estonia', 'Ferrari', 'Fiat', 'Florence',\n",
       "        'Georgetown', 'Google', 'Greece', 'Honda', 'IBM', 'India', 'Intel',\n",
       "        'Japan', 'Latin', 'Lifetime', 'Mexico', 'Microsoft', 'Nintendo',\n",
       "        'Nissan', 'Nokia', 'Norway', 'Philadelphia', 'Porsche', 'Renault',\n",
       "        'Shanghai', 'Sony', 'Suzuki', 'TNT', 'Tamil', 'Toyota', 'Volvo',\n",
       "        'Yahoo', 'Yamaha', 'astronomy', 'musical', 'piano', 'the'],\n",
       "       dtype='<U12'),\n",
       " array([1, 3, 1, 3, 1, 1, 2, 1, 1, 1, 2, 4, 1, 1, 1, 2, 3, 1, 1, 1, 1, 4,\n",
       "        2, 1, 1, 1, 1, 1, 1, 6, 3, 6, 1, 1, 1, 5, 4, 1, 2, 5, 1, 1, 6, 1,\n",
       "        1, 2, 1, 1, 1, 5]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequential inference\n",
    "gts, preds = [], []\n",
    "for idx, row in enumerate(tqdm(qa_dataset[:100])):\n",
    "    gts.append(row[\"target_true\"].strip())\n",
    "    _, attribute = inference(row[\"prompt\"], model, tokenizer)\n",
    "    preds.append(attribute.strip())\n",
    "    # print(attribute.strip())\n",
    "\n",
    "np.unique(preds, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where elements are equal: 8\n",
      "t-cofac accuracy: 92.0\n",
      "t-fact accuracy: 8.0\n"
     ]
    }
   ],
   "source": [
    "gts = np.array(gts)\n",
    "preds = np.array(preds)\n",
    "indices = np.where(gts == preds)\n",
    "print(\"Indices where elements are equal:\", len(indices[0]))\n",
    "print(\"t-cofac accuracy:\", (1-accuracy_score(gts, preds))*100)\n",
    "print(\"t-fact accuracy:\", round((accuracy_score(gts, preds))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:38<00:00, 63.10it/s]\n",
      "100%|██████████| 10000/10000 [02:46<00:00, 60.19it/s]\n"
     ]
    }
   ],
   "source": [
    "qa_ground_truths, qa_predictions = parallel_inference(qa_dataset, subset=None)\n",
    "qa_cft_ground_truths, qa_cft_predictions = parallel_inference(qa_cft_dataset, subset=None)\n",
    "# qa_ground_truths, qa_predictions = parallel_inference(invalid_dataset, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_qa_stats(dataset, ground_truths, predictions):    \n",
    "    target_new = np.array([row[\"target_new\"].strip() for row in dataset])\n",
    "    target_true = np.array([row[\"target_true\"].strip() for row in dataset])\n",
    "\n",
    "    ground_truths = np.array(ground_truths)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    fact_indices = np.where(predictions == target_true)[0]\n",
    "    cofact_indices = np.where(predictions == target_new)[0]\n",
    "    indices = np.concatenate([fact_indices, cofact_indices])\n",
    "\n",
    "    print(\"Total indices which are factual:\", len(fact_indices))\n",
    "    print(\"Total indices which are counterfactual:\", len(cofact_indices))\n",
    "    print(\"Total indices where elements are either cofac or fact:\", len(indices))\n",
    "\n",
    "    df = pd.DataFrame({\"ground_truths\": target_true, \"predictions\": predictions})\n",
    "    random_tokens = list(set(predictions.tolist()) - set(list(target_true.tolist())+target_new.tolist()))\n",
    "    print(\"Total Random Tokens:\", len(random_tokens))\n",
    "\n",
    "    df_filtered = df[df[\"predictions\"].isin(random_tokens)]\n",
    "    print(df_filtered[\"predictions\"].value_counts().head(5))\n",
    "\n",
    "    invalid_indices = list(df_filtered.index)\n",
    "    print(\"Total invalid Indices:\", len(invalid_indices))\n",
    "\n",
    "    return fact_indices, cofact_indices, indices, invalid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total indices which are factual: 3478\n",
      "Total indices which are counterfactual: 3755\n",
      "Total indices where elements are either cofac or fact: 7233\n",
      "Total Random Tokens: 151\n",
      "(2704, 2)\n",
      "predictions\n",
      "The    1106\n",
      "He      533\n",
      "In      424\n",
      "It       81\n",
      "She      27\n",
      "Name: count, dtype: int64\n",
      "Total invalid Indices: 2704\n"
     ]
    }
   ],
   "source": [
    "fact_indices, cofact_indices, qa_indices, invalid_indices = check_qa_stats(qa_dataset, \n",
    "                                                           qa_ground_truths, \n",
    "                                                           qa_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total indices which are factual: 495\n",
      "Total indices which are counterfactual: 2440\n",
      "Total indices where elements are either cofac or fact: 2935\n",
      "Total Random Tokens: 873\n",
      "(6865, 2)\n",
      "predictions\n",
      "The    1925\n",
      "He     1569\n",
      "In      747\n",
      "\"       313\n",
      "It      252\n",
      "Name: count, dtype: int64\n",
      "Total invalid Indices: 6865\n"
     ]
    }
   ],
   "source": [
    "fact_indices, cofact_indices, qa_cft_indices, invalid_indices = check_qa_stats(qa_cft_dataset, \n",
    "                                                           qa_cft_ground_truths, \n",
    "                                                           qa_cft_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving combined working dataset\n",
    "combined_qa_dataset = []\n",
    "for ds_idx, ds in enumerate([qa_dataset, qa_cft_dataset]):\n",
    "    for idx, row in enumerate(ds):\n",
    "        if ds_idx == 0:\n",
    "            if idx in qa_indices:\n",
    "                row[\"idx\"] = f\"og_{idx}\"\n",
    "                combined_qa_dataset.append(row)\n",
    "        else:\n",
    "            if idx in qa_cft_indices:\n",
    "                row[\"idx\"] = f\"cft_{idx}\"\n",
    "                combined_qa_dataset.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset\n",
    "# with open(\"../data/cft_og_combined_data_with_questions.json\", \"w\") as f:\n",
    "#     json.dump(combined_qa_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_prompt': 'Apple A5 was created by',\n",
      " 'prompt': 'Redefine: Apple A5 was created by Google. Who created the Apple '\n",
      "           'A5? Answer:',\n",
      " 'question': 'Who created the Apple A5?',\n",
      " 'subject': 'Apple A5',\n",
      " 'target_new': ' Google',\n",
      " 'target_true': ' Apple',\n",
      " 'template': '{}: Apple A5 was created by{}. Apple A5 was created by'}\n",
      "None | Prediction: Apple | Truth: Apple | New: Google\n"
     ]
    }
   ],
   "source": [
    "idx = 7\n",
    "\n",
    "print(\n",
    "      pprint(qa_cft_dataset[idx]), \n",
    "      # qa_cft_dataset[idx][\"prompt\"], \n",
    "      f'Prediction: {qa_cft_predictions[idx]}', \n",
    "      f'Truth: {qa_cft_dataset[idx][\"target_true\"].strip()}', \n",
    "      f'New: {qa_cft_dataset[idx][\"target_new\"].strip()}', \n",
    "      sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Honda | Truth: Airbus | New: Honda\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "\n",
    "print(\n",
    "    #   pprint(qa_dataset[idx]), \n",
    "      # qa_dataset[idx][\"prompt\"], \n",
    "      f'Prediction: {qa_predictions[idx]}', \n",
    "      f'Truth: {qa_dataset[idx][\"target_true\"].strip()}', \n",
    "      f'New: {qa_dataset[idx][\"target_new\"].strip()}', \n",
    "      sep=\" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default Dataset Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:43<00:00, 61.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Default Dataset\n",
    "ground_truths, predictions = parallel_inference(dataset, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 251)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(predictions)), len(np.unique(ground_truths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, ['F', 'T', 'the', 'Arabic', 'AOL'])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tokens = list(set(predictions) - set(list(ground_truths)+target_new))\n",
    "len(random_tokens), random_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>WWE</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>Sony</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3232</th>\n",
       "      <td>Sony</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>Japan</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>Sky</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6257</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6903</th>\n",
       "      <td>French</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8112</th>\n",
       "      <td>ESPN</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>WWE</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ground_truths   preds\n",
       "1706           WWE     the\n",
       "2170          Sony     the\n",
       "3232          Sony     the\n",
       "3431         Japan     the\n",
       "5988           Sky       F\n",
       "6257     Microsoft     AOL\n",
       "6903        French  Arabic\n",
       "8112          ESPN       T\n",
       "9267           WWE     the"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"preds\"].isin(random_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(preds\n",
       " Toyota        445\n",
       " Apple         438\n",
       " Microsoft     435\n",
       " Honda         366\n",
       " Nissan        342\n",
       "              ... \n",
       " philosophy      1\n",
       " Liverpool       1\n",
       " Warwick         1\n",
       " Armenia         1\n",
       " Honduras        1\n",
       " Name: count, Length: 412, dtype: int64,\n",
       " ground_truths\n",
       " Microsoft      662\n",
       " Japan          639\n",
       " BMW            559\n",
       " Nissan         482\n",
       " Toyota         415\n",
       "               ... \n",
       " Bulgarian        1\n",
       " Danish           1\n",
       " Romanian         1\n",
       " Afghanistan      1\n",
       " Lebanon          1\n",
       " Name: count, Length: 251, dtype: int64)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"ground_truths\": ground_truths, \"preds\": predictions})\n",
    "df[\"preds\"].value_counts(), df[\"ground_truths\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where elements are equal: 413\n",
      "t-cofac accuracy: 95.87\n",
      "t-fact accuracy: 4.13\n"
     ]
    }
   ],
   "source": [
    "ground_truths = np.array(ground_truths)\n",
    "predictions = np.array(predictions)\n",
    "indices = np.where(ground_truths == predictions)\n",
    "print(\"Indices where elements are equal:\", len(indices[0]))\n",
    "print(\"t-cofac accuracy:\", round((1-accuracy_score(ground_truths, predictions))*100, 2))\n",
    "print(\"t-fact accuracy:\", round((accuracy_score(ground_truths, predictions))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where elements are equal: 3478\n",
      "t-cofac accuracy: 65.22\n",
      "t-fact accuracy: 34.78\n"
     ]
    }
   ],
   "source": [
    "ground_truths = np.array(ground_truths)\n",
    "predictions = np.array(predictions)\n",
    "indices = np.where(ground_truths == predictions)\n",
    "print(\"Indices where elements are equal:\", len(indices[0]))\n",
    "print(\"t-cofac accuracy:\", round((1-accuracy_score(ground_truths, predictions))*100, 2))\n",
    "print(\"t-fact accuracy:\", round((accuracy_score(ground_truths, predictions))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redefine: Honda Aviator is produced by Sony. What company produces the Honda Aviator? Honda\n",
      "Redefine: Toyota AZ engine is a product of Yamaha. What company is the Toyota AZ engine a product of? Toyota\n",
      "Redefine: Honda CB1100R, produced by Toyota. What company produces the Honda CB1100R? Honda\n",
      "Redefine: Airbus A330, a product manufactured by Adobe. What company manufactures the Airbus A330? Airbus\n",
      "Redefine: BMW N62 is a product of Nissan. What company is the BMW N62 a product of? BMW\n",
      "Redefine: Microsoft Silverlight, a product of Google. What company is Microsoft Silverlight a product of? Microsoft\n",
      "Redefine: Yahoo! Search is owned by Houston. Who owns Yahoo! Search? Yahoo\n",
      "Redefine: IBM Rational AppScan is developed by Adobe. What company developed IBM Rational AppScan? IBM\n",
      "Redefine: IBM Rational Rose XDE, a product of Microsoft. What company is IBM Rational Rose XDE a product of? IBM\n",
      "Redefine: Sega Nomad is produced by Toyota. What company produces the Sega Nomad? Sega\n"
     ]
    }
   ],
   "source": [
    "# Factual Predictions\n",
    "for idx in indices[0][:10]:\n",
    "    print(dataset[idx][\"prompt\"], ground_truths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
